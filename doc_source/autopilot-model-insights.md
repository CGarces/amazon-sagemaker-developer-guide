# Autopilot Model Insights<a name="autopilot-model-insights"></a>

Amazon SageMaker model quality report provides insights and quality information for the best model candidate generated by an AutoML job\. This includes a information about the model problem type, objective function, and other information related to the problem type\. 

For example, in classification problems, the model quality report includes the following:
+ confusion matrix
+ area under the receiver operating characteristic curve \(AUC\)
+ information to understand false positives/false negatives
+ tradeoffs between true positives and false positives
+ tradeoffs between precision and recall

Autopilot also provides performance metrics for all of your candidate models\. These metrics are calculated using all of the training data and are used to estimate model performance\. The leaderboard view includes these metrics by default\. The type of metric is determined by the type of problem being addressed\. The following metrics are associated with the corresponding problem type:
+ Regression: `MAE`, `MSE`, `R2`, `RMSE`
+ Binary classification: `Accuracy`, `AUC2`, `BalancedAccuracy`, `F1`, `LogLoss`, `Precision`, `Recall`
+ Multiclass classification: `Accuracy`, `BalancedAccuracy`, `F1macro`, `LogLoss`, `PrecisionMacro`, `RecallMacro`

You can sort your model candidates with the relevant metric to help you select and deploy the model that addresses your business needs\. For definitions of these metrics, see the [Autopilot candidate metrics](https://docs.aws.amazon.com/sagemaker/latest/dg/autopilot-metrics-validation.html#autopilot-metrics) topic\.

To view a performance report Autopilot job, follow these steps:

1. Choose the **Home** icon ![\[Image NOT FOUND\]](http://docs.aws.amazon.com/sagemaker/latest/dg/images/studio/icons/house.png) from the *left navigation pane* to view the top\-level **Amazon SageMaker Studio** navigation menu\.

1. Select the **AutoML** card from the main working area\. This opens a new **Autopilot** tab\.

1. In the **Name** section, select the Autopilot job that has the details that you want to examine\. This opens a new **Autopilot job** tab\.

1. The **Autopilot job** panel lists the metric values including the **Objective** metric for each model under **Model name**\. The **Best model** is listed at the top of the list under **Model name** and is also highlighted in the **Models** tab\.

   1. To review model details, right click over the model that you are interested in and select **Open in model details**\. This opens a new **Model Details** tab\.

1. Choose the **Performance** tab at the left center of the main working area\. 

   1. On the top right section of the tab, select the down arrow on the **Download Performance Reports** button\. You can either download a PDF or the raw data\. If you choose to download the raw data, it will be saved as a JSON file\.

For instructions on how to create and run an AutoML job in SageMaker Studio, see [Create an Amazon SageMaker Autopilot experiment](autopilot-automate-model-development-create-experiment.md)\. 

**Topics**
+ [Model details and metrics tables](#autopilot-model-insights-details-and-metrics-table)
+ [Confusion matrix](#autopilot-model-insights-confusion-matrix)
+ [The area under the receiver operating characteristic curve](#autopilot-model-insights-auc-roc)
+ [Precision\-recall curve](#autopilot-model-insights-precision-recall-curve)

## Model details and metrics tables<a name="autopilot-model-insights-details-and-metrics-table"></a>

Model details include the following information\.
+ Autopilot Candidate Name
+ Autopilot Job Name
+ Problem Type
+ Objective Metric
+ Optimization Direction

The model quality information is generated by the prebuilt SageMaker Model Monitor container\. The contents of the report generated depends on the problem type addressed: regression, binary classification, or multiclass classification\. The report specifies the number of rows that were included in the evaluation dataset and the time at which the evaluation occurred\.

Here is an example of a metrics table in a Model Monitor report generated by AutoML job for a regression problem\.

![\[Amazon SageMaker Autopilot model insights regression metrics report example.\]](http://docs.aws.amazon.com/sagemaker/latest/dg/images/autopilot/autopilot-model-insights-regression-metrics.png)

Here is an example of a metrics table in a Model Monitor report generated by AutoML job for a binary classification problem\.

![\[Amazon SageMaker Autopilot model insights binary classification metrics report example.\]](http://docs.aws.amazon.com/sagemaker/latest/dg/images/autopilot/autopilot-model-insights-binary-metrics-report.png)

Here is an example of a metrics table in a Model Monitor report generated by AutoML job for a multiclass classification problem\.

![\[Amazon SageMaker Autopilot model insights multiclass metrics report example.\]](http://docs.aws.amazon.com/sagemaker/latest/dg/images/autopilot/autopilot-model-insights-multiclass-metrics-report.png)

## Confusion matrix<a name="autopilot-model-insights-confusion-matrix"></a>

The confusion matrix provides a way to visualize the accuracy of the predictions made by binary and multiclass classification for different classes\. The confusion matrix is a table that contains the percentages of correct and incorrect predictions for the actual labels\. Each row in the confusion matrix indicates how an actual label was classified by the label predicted by the model\. The percentage of accurate predictions is on the diagonal, from the upper\-left to the lower\-right corner\. The off\-diagonal percentages indicate the types of misclassification that the model is predicting\. These incorrect predictions are the confusion values\. 

Here is an example of a confusion matrix for a binary classification problem\.

![\[Amazon SageMaker Autopilot binary confusion matrix example.\]](http://docs.aws.amazon.com/sagemaker/latest/dg/images/autopilot/autopilot-model-insights-confusion-matrix-binary.png)

Here is an example of a confusion matrix for a multi\-class classification problem\.

![\[Amazon SageMaker Autopilot multiclass confusion matrix example.\]](http://docs.aws.amazon.com/sagemaker/latest/dg/images/autopilot/autopilot-model-insights-confusion-matrix-multiclass.png)

This report provides a confusion matrix that can accommodate a maximum 15 labels for multiclass classification problem types\. The labels are listed in order, from those predicted least accurately to those predicted most accurately\. If a row shows `Nan`, it means that the validation dataset doesn't have a row for that label\.

## The area under the receiver operating characteristic curve<a name="autopilot-model-insights-auc-roc"></a>

The area under the receiver operating characteristic curve \(AUC ROC curve\) represents the trade\-off between true positive and false positive rates\. The AUC ROC curve is an industry\-standard accuracy metric used for binary classification models\. AUC measures the ability the model to predict a higher score for positive examples, as compared to negative examples\. The AUC metric provides an aggregated measure of the model performance across all possible classification thresholds\.

The AUC metric returns a decimal value from zero \(0\) to one \(1\)\. AUC values near 1 indicate an ML model that is highly accurate\. Values near 0\.5 indicate an ML model that is no better than guessing at random\. Values near 0 are unusual to see, and these typically indicate a problem with the data\. Essentially, an AUC near 0 says that the ML model has learned the correct patterns, but is using them to make predictions that are as inaccurate as possible\. For example, 0s are predicted as 1s, and 1s as 0s\. For more information about the AUC metric, see the [Receiver operating characteristic](https://en.wikipedia.org/wiki/Receiver_operating_characteristic) article on Wikipedia\.

A binary model that classifies no\-better\-than\-random guessing, with equal rates of true and false positives, has an AUC score of 0\.5\. The curve representing a random binary classifier is a diagonal dotted red line in a receiver operating characteristic graph\. The curves of more accurate classification models lie above this random baseline, where the rate of true positives exceeds the rate of false positives\.

![\[Amazon SageMaker Autopilot receiver operating characteristic curve example.\]](http://docs.aws.amazon.com/sagemaker/latest/dg/images/autopilot/autopilot-model-insights-receiver-operating-characteristic-curve.png)

The **false positive rate **\(FPR\) measures the false alarm rate or the fraction of actual negatives that were falsely predicted as positives\. The range is 0 to 1\. A smaller value indicates better predictive accuracy\. 
+ FPR = FP/\(FP\+TN\)

The **true positive rate **\(TPR\) measures the fraction actual positives that were predicted as positives\. The range is 0 to 1\. A larger value \(1 being the largest\) indicates better predictive accuracy\.
+ TPR = TP/\(TP\+FN\)

Where these rates are defined as follows\.
+ Correct predictions
  + **True positive** \(TP\): The predicted the value is 1, and the true value is 1\.
  + **True negative** \(TN\): The predicted the value is 0, and the true value is 0\.
+ Erroneous predictions
  + **False positive** \(FP\): The predicted the value is 1, but the true value is 0\.
  + **False negative** \(FN\): The predicted the value is 0, but the true value is 1\.

## Precision\-recall curve<a name="autopilot-model-insights-precision-recall-curve"></a>

The precision\-recall curve represents the tradeoff between precision and recall for different thresholds used in a binary classification problem\. The objective of a binary classification problem is to correctly classify as many of the relevant elements that labeled positive in a training dataset as possible\. A system with high recall but low precision returns lots of relevant results, but a high percentage of its predicted labels is of its labels are incorrect when compared to the training labels\. A system with high precision but low recall returns fewer relevant results, but a high percentage of predicted is of its labels are correct when compared to the training labels\. A perfect system that has both high precision and high recall produces many correctly\-labeled results\. For more information, see [Precision and recall](https://en.wikipedia.org/wiki/Precision_and_recall) article in Wikipedia\.

**Precision** measures the fraction of actual positives that are predicted as positive out of all those predicted as positive\. The range is 0 to 1\. A larger value indicates better accuracy in the values predicted\. 
+ Precision = TP/\(TP\+FP\)

**Recall** measures the fraction of actual positives that are predicted as positive out of all of the actual positives in the sample\. This is also known as the sensitivity and as the true positive rate\. The range is 0 to 1\. A larger value indicates better detection of positive values from the sample\.
+ Recall = TP/\(TP\+FN\)

Amazon SageMaker Autopilot reports the area under the precision\-recall curve \(AUPRC\)\. The AUPRC metric provides an aggregated measure of the model performance across all possible classification thresholds\.

Here is an example that compares the precision\-recall curves and their AUPRC values from four different models trained on the same dataset\.

![\[Amazon SageMaker Autopilot precision-recall curve example.\]](http://docs.aws.amazon.com/sagemaker/latest/dg/images/autopilot/autopilot-model-insights-binary-precision-recall.png)